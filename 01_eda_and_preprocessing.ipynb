{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb92d78",
   "metadata": {},
   "source": [
    "# Derma Scanner — EDA & Preprocessing\n",
    "\n",
    "> Explore HAM10000, verify splits, visualize samples, and review preprocessing steps.\n",
    "\n",
    "**Note:** Run this from the project root after preparing splits (`python src/data_prepare.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffb1334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random, os\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "PROJECT = Path('.')\n",
    "RAW = PROJECT/'data/raw'\n",
    "PROC = PROJECT/'data/processed'\n",
    "assert PROC.exists(), 'Run: python src/data_prepare.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf560f3",
   "metadata": {},
   "source": [
    "## Class distribution in processed splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5acd2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(split_dir):\n",
    "    counts = {}\n",
    "    for cls in sorted(os.listdir(split_dir)):\n",
    "        p = split_dir/cls\n",
    "        if p.is_dir():\n",
    "            counts[cls] = len([f for f in p.glob('*.jpg')])\n",
    "    return counts\n",
    "\n",
    "for split in ['train','val','test']:\n",
    "    counts = count_images(PROC/split)\n",
    "    print(split, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9586630",
   "metadata": {},
   "source": [
    "## Visualize a few samples per class (from `data/processed/train`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8643ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "classes = [d.name for d in (PROC/'train').iterdir() if d.is_dir()]\n",
    "samples = []\n",
    "for cls in classes:\n",
    "    files = list((PROC/'train'/cls).glob('*.jpg'))\n",
    "    if files:\n",
    "        samples.append((cls, random.choice(files)))\n",
    "\n",
    "cols = 4\n",
    "rows = math.ceil(len(samples)/cols)\n",
    "plt.figure(figsize=(4*cols, 3*rows))\n",
    "for i,(cls,fp) in enumerate(samples,1):\n",
    "    plt.subplot(rows, cols, i)\n",
    "    img = Image.open(fp)\n",
    "    plt.imshow(img)\n",
    "    plt.title(cls)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de0dfd1",
   "metadata": {},
   "source": [
    "## Preprocessing used in the pipeline\n",
    "We apply the following steps:\n",
    "- **Resize** to 224×224\n",
    "- **Normalize** to ImageNet stats (`mean=[0.485, 0.456, 0.406]`, `std=[0.229, 0.224, 0.225]`)\n",
    "- **Data augmentation (train only):** random horizontal/vertical flips and light color jitter\n",
    "\n",
    "These are implemented in `src/train.py` via torchvision `transforms`. You can tune them here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "print(train_transforms)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
